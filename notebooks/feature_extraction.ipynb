{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5602e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc8c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f369914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, sr=22050, frame_size=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.frame_size = frame_size\n",
    "        self.hop_length = hop_length\n",
    "        self.spectrogram = None  # Cache spectrogram for reuse\n",
    "\n",
    "    def compute_spectrogram(self, audio):\n",
    "        \"\"\"Compute spectrogram once and cache it.\"\"\"\n",
    "        if self.spectrogram is None:\n",
    "            self.spectrogram = librosa.stft(\n",
    "                audio, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "            )\n",
    "        return self.spectrogram\n",
    "\n",
    "    def extract_features(self, audio):\n",
    "        features = {}\n",
    "\n",
    "        # Time domain features\n",
    "        features[\"zcr\"] = np.mean(self.extract_zcr(audio))\n",
    "        features[\"rms\"] = np.mean(self.extract_rms(audio))\n",
    "        features[\"amplitude_envelope\"] = np.mean(self.extract_amplitude_envelope(audio))\n",
    "\n",
    "        # Frequency domain features\n",
    "        features[\"spectral_centroid\"] = np.mean(self.extract_spectral_centroid(audio))\n",
    "        features[\"spectral_bandwidth\"] = np.mean(self.extract_spectral_bandwidth(audio))\n",
    "        features[\"band_energy_ratio\"] = np.mean(\n",
    "            self.band_energy_ratio(audio, split_frequency=1000)\n",
    "        )\n",
    "        features[\"spectral_rolloff\"] = np.mean(self.extract_spectral_rolloff(audio))\n",
    "        features[\"spectral_flatness\"] = np.mean(self.extract_spectral_flatness(audio))\n",
    "        features[\"mfcc\"] = self.extract_mfcc(audio)\n",
    "        features[\"mel_spectrogram\"] = np.mean(self.mel_spectrogram(audio))\n",
    "        features[\"spectrogram\"] = np.mean(self.extract_spectrogram(audio))\n",
    "        \n",
    "        # Statistical features\n",
    "        features[\"cqt\"] = np.mean(self.extract_cqt(audio))\n",
    "        features[\"pitch\"] = np.mean(self.extract_pitch(audio))\n",
    "        features[\"tempo\"] = self.extract_tempo(audio)\n",
    "        features[\"beat_interval\"] = self.extract_beat_interval(audio)\n",
    "        features[\"onset_interval\"] = self.extract_onset_interval(audio)\n",
    "\n",
    "        self.spectrogram = None  # Clear cached spectrogram\n",
    "        return features\n",
    "\n",
    "    def extract_zcr(self, audio):\n",
    "        zcr = librosa.feature.zero_crossing_rate(\n",
    "            audio, frame_length=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return zcr if zcr.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_rms(self, audio):\n",
    "        rms = librosa.feature.rms(\n",
    "            y=audio, frame_length=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return rms if rms.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_amplitude_envelope(self, audio):\n",
    "        amplitude_envelope = np.array(\n",
    "            [\n",
    "                max(audio[i : i + self.frame_size])\n",
    "                for i in range(0, len(audio), self.hop_length)\n",
    "            ]\n",
    "        )\n",
    "        return amplitude_envelope if amplitude_envelope.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_spectral_centroid(self, audio):\n",
    "        centroid = librosa.feature.spectral_centroid(\n",
    "            y=audio, sr=self.sr, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return centroid if centroid.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_spectral_bandwidth(self, audio):\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(\n",
    "            y=audio, sr=self.sr, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return bandwidth if bandwidth.size > 0 else np.array([0.0])\n",
    "\n",
    "    def band_energy_ratio(self, audio, split_frequency=1000):\n",
    "        spectrogram = self.compute_spectrogram(audio)\n",
    "\n",
    "        num_frequency_bins = self.frame_size // 2 + 1\n",
    "        frequency_range = self.sr / 2\n",
    "        frequency_delta_per_bin = frequency_range / num_frequency_bins\n",
    "        split_frequency_bin = int(math.floor(split_frequency / frequency_delta_per_bin))\n",
    "\n",
    "        power_spectrogram = np.abs(spectrogram) ** 2\n",
    "        power_spectrogram = power_spectrogram.T\n",
    "\n",
    "        band_energy_ratio = []\n",
    "        for frame in power_spectrogram:\n",
    "            sum_power_low = frame[:split_frequency_bin].sum()\n",
    "            sum_power_high = frame[split_frequency_bin:].sum()\n",
    "            ber = sum_power_low / (sum_power_high + 1e-10)\n",
    "            band_energy_ratio.append(ber)\n",
    "\n",
    "        ber_array = np.array(band_energy_ratio)\n",
    "        return ber_array if ber_array.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_spectral_rolloff(self, audio):\n",
    "        rolloff = librosa.feature.spectral_rolloff(\n",
    "            y=audio, sr=self.sr, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return rolloff if rolloff.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_spectral_contrast(self, audio):\n",
    "        contrast = librosa.feature.spectral_contrast(\n",
    "            y=audio, sr=self.sr, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )\n",
    "        return np.mean(contrast, axis=0) if contrast.size > 0 else np.zeros(7)\n",
    "\n",
    "    def extract_spectral_flatness(self, audio):\n",
    "        flatness = librosa.feature.spectral_flatness(\n",
    "            y=audio, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )[0]\n",
    "        return flatness if flatness.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_spectrogram(self, audio):\n",
    "        spectrogram = librosa.amplitude_to_db(\n",
    "            self.compute_spectrogram(audio), ref=np.max\n",
    "        )\n",
    "        return (\n",
    "            spectrogram\n",
    "            if spectrogram.size > 0\n",
    "            else np.zeros((self.frame_size // 2 + 1, 1))\n",
    "        )\n",
    "\n",
    "    def mel_spectrogram(self, audio):\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sr, hop_length=self.hop_length, n_mels=128\n",
    "        )\n",
    "        return mel_spec[0] if mel_spec.size > 0 else np.zeros(1)\n",
    "\n",
    "    def extract_mfcc(self, audio, n_mfcc=13):\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=audio,\n",
    "            sr=self.sr,\n",
    "            n_fft=self.frame_size,\n",
    "            hop_length=self.hop_length,\n",
    "            n_mfcc=n_mfcc,\n",
    "        )\n",
    "        return np.mean(mfccs, axis=1) if mfccs.size > 0 else np.zeros(n_mfcc)\n",
    "\n",
    "    def extract_chroma(self, audio):\n",
    "        chroma = librosa.feature.chroma_stft(\n",
    "            y=audio, sr=self.sr, n_fft=self.frame_size, hop_length=self.hop_length\n",
    "        )\n",
    "        return np.mean(chroma, axis=1) if chroma.size > 0 else np.zeros(12)\n",
    "\n",
    "    def extract_cqt(self, audio):\n",
    "        cqt = librosa.cqt(y=audio, sr=self.sr, hop_length=self.hop_length)\n",
    "        return np.abs(cqt) if cqt.size > 0 else np.zeros((84, 1))\n",
    "\n",
    "    def extract_pitch(self, audio):\n",
    "        pitches, magnitudes = librosa.piptrack(y=audio, sr=self.sr)\n",
    "        # Extract pitch values where magnitude is non-zero\n",
    "        pitch_values = pitches[magnitudes > 0]\n",
    "        return pitch_values if pitch_values.size > 0 else np.array([0.0])\n",
    "\n",
    "    def extract_tonnetz(self, audio):\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=self.sr)\n",
    "        return np.mean(tonnetz, axis=1) if tonnetz.size > 0 else np.zeros(6)\n",
    "\n",
    "    def extract_tempo(self, audio):\n",
    "        tempo = librosa.beat.tempo(y=audio, sr=self.sr)[0]\n",
    "        return tempo if not np.isnan(tempo) else 0.0\n",
    "\n",
    "    def extract_beat_interval(self, audio):\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=audio, sr=self.sr)\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=self.sr)\n",
    "        if len(beat_times) < 2:\n",
    "            return 0.0\n",
    "        intervals = np.diff(beat_times)\n",
    "        return np.mean(intervals) if intervals.size > 0 else 0.0\n",
    "\n",
    "    def extract_onset_interval(self, audio):\n",
    "        onset_frames = librosa.onset.onset_detect(y=audio, sr=self.sr)\n",
    "        onset_times = librosa.frames_to_time(onset_frames, sr=self.sr)\n",
    "        if len(onset_times) < 2:\n",
    "            return 0.0\n",
    "        intervals = np.diff(onset_times)\n",
    "        return np.mean(intervals) if intervals.size > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b441b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, columns):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        audio, sr = librosa.load(file_path, sr=None, mono=True, duration=5.0)\n",
    "        logging.info(f\"Loaded {file_path} in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        if len(audio) < sr:\n",
    "            logging.warning(\n",
    "                f\"Skipping {file_path}: Audio too short ({len(audio)/sr:.2f} seconds)\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        start_time = time.time()\n",
    "        extractor = FeatureExtractor(sr=sr)\n",
    "        features = extractor.extract_features(audio)\n",
    "        logging.info(\n",
    "            f\"Extracted features from {file_path} in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        # Check for NaN values in features\n",
    "        for key, val in features.items():\n",
    "            if isinstance(val, (np.ndarray, float, int)):\n",
    "                if isinstance(val, np.ndarray):\n",
    "                    if np.isnan(val).any():\n",
    "                        logging.warning(\n",
    "                            f\"Skipping {file_path}: Features contain NaN values in {key}\"\n",
    "                        )\n",
    "                        return None\n",
    "                else:\n",
    "                    if np.isnan(val):\n",
    "                        logging.warning(\n",
    "                            f\"Skipping {file_path}: Features contain NaN values in {key}\"\n",
    "                        )\n",
    "                        return None\n",
    "\n",
    "        # Flatten array-like features (mfcc, spectral_contrast, chroma, tonnetz) into separate columns\n",
    "        flattened_features = {}\n",
    "        for key, val in features.items():\n",
    "            if isinstance(val, np.ndarray) and val.size > 1:  # For array-like features\n",
    "                for i, v in enumerate(val):\n",
    "                    flattened_features[f\"{key}_{i}\"] = v\n",
    "            else:\n",
    "                flattened_features[key] = val\n",
    "\n",
    "        return flattened_features\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8035bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory_path, label, batch_size=100, n_jobs=4):\n",
    "    audio_files = [\n",
    "        os.path.join(directory_path, f)\n",
    "        for f in os.listdir(directory_path)\n",
    "        if f.endswith((\".wav\", \".mp3\"))\n",
    "    ]\n",
    "    logging.info(f\"Found {len(audio_files)} audio files in {directory_path}\")\n",
    "\n",
    "    sample_file = audio_files[0] if audio_files else None\n",
    "    if not sample_file:\n",
    "        logging.warning(f\"No audio files found in {directory_path}\")\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    sample_features = extract_features(sample_file, columns=None)\n",
    "    if sample_features is None:\n",
    "        logging.warning(f\"Could not determine columns from {sample_file}\")\n",
    "        return\n",
    "    columns = list(sample_features.keys())\n",
    "    logging.info(\n",
    "        f\"Determined {len(columns)} columns in {time.time() - start_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(audio_files), batch_size):\n",
    "        batch_files = audio_files[i : i + batch_size]\n",
    "        logging.info(\n",
    "            f\"Processing batch {i // batch_size + 1} with {len(batch_files)} files\"\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "            delayed(extract_features)(file_path, columns) for file_path in batch_files\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Processed batch {i // batch_size + 1} in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        filtered_res = [item for item in results if item is not None]\n",
    "        if not filtered_res:\n",
    "            logging.warning(f\"No valid results in batch {i // batch_size + 1}\")\n",
    "            continue\n",
    "\n",
    "        for res in filtered_res:\n",
    "            res[\"label\"] = label\n",
    "\n",
    "        start_time = time.time()\n",
    "        df = pd.DataFrame(filtered_res, columns=columns + [\"label\"])\n",
    "        logging.info(\n",
    "            f\"Created DataFrame with {len(df)} rows in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        logging.info(\n",
    "            f\"Dropped {initial_rows - len(df)} rows with NaN values in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        if df.empty:\n",
    "            logging.warning(\n",
    "                f\"Batch {i // batch_size + 1} is empty after dropping NaN values\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        output_file = f\"{directory_path}/batch_{i // batch_size + 1}_features.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(\n",
    "            f\"Saved batch {i // batch_size + 1} with {len(df)} entries to {output_file} in {time.time() - start_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        del df, filtered_res, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"processed\"\n",
    "\n",
    "labels = {\n",
    "    \"Female_Fifties\": 0,\n",
    "    \"Female_Twenties\": 1,\n",
    "    \"Male_Fifties\": 2,\n",
    "    \"Male_Twenties\": 3,\n",
    "}\n",
    "batch_size = 100  # Keep small for initial timing\n",
    "n_jobs = -1\n",
    "\n",
    "for dir_name, label in labels.items():\n",
    "    dir_path = os.path.join(base_dir, dir_name)\n",
    "    if os.path.exists(dir_path):\n",
    "        logging.info(f\"Processing directory: {dir_path} with label {label}\")\n",
    "        process_directory(dir_path, label, batch_size, n_jobs)\n",
    "    else:\n",
    "        logging.warning(f\"Directory {dir_path} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
